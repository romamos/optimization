# Optimize computer vision models
Практика по курсу "Оптимизация моделей компьютерного зрения"

# Задание 2

На данных графиках представлена динамика изменения потерь для двух различных алгоритмов обучения: Rprop и SGD (Stochastic Gradient Descent).

### Rprop:
- На первой эпохе наблюдается относительно низкая потеря, но к четвертой эпохе она достигает своего пика. После этого на пятой эпохе потеря снова уменьшается. Такое непоследовательное поведение может быть связано с тем, что Rprop, фокусируясь на знаках градиента, мог игнорировать величину градиента, что в некоторых случаях приводит к нестабильности.
- Особенности алгоритма: Rprop адаптирует шаг на основе изменений знака градиента, что может помочь на большом кол-ве эпох, но иногда приводит к колебаниям в начале обучения.


### SGD:
- Демонстрирует стабильное и последовательное уменьшение потерь на каждой эпохе. 
- Постепенное и последовательное уменьшение потерь свидетельствует о надежности и предсказуемости SGD.
- Хорошо подходит для задач, где стабильность и предсказуемость важны, особенно на начальных этапах обучения.

### Выводы
- Предпочтительность алгоритма: Исходя из графиков, если основная цель — максимальная стабильность и предсказуемость при снижении потерь на небольшом числе эпох, предпочтительным вариантом станет SGD. Однако если задача требует более тонкой настройки и допускает колебания на начальных этапах, Rprop может оказаться подходящим.

![loss_plot_Rprop.png](loss_plot_Rprop.png)
![loss_plot_SGD.png](loss_plot_SGD.png)




