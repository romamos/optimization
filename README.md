# Optimize computer vision models
Практика по курсу "Оптимизация моделей компьютерного зрения"

# Задание 2

На данных графиках представлена динамика изменения потерь для двух различных алгоритмов обучения: Rprop и SGD (Stochastic Gradient Descent).

### Rprop:
- На первой эпохе наблюдается относительно низкая потеря, но к четвертой эпохе она достигает своего пика. После этого на пятой эпохе потеря снова уменьшается. Такое непоследовательное поведение может быть связано с тем, что Rprop, фокусируясь на знаках градиента, мог игнорировать величину градиента, что в некоторых случаях приводит к нестабильности.
- Особенности алгоритма: Rprop адаптирует шаг на основе изменений знака градиента, что может помочь на большом кол-ве эпох, но иногда приводит к колебаниям в начале обучения.


### SGD:
- Демонстрирует стабильное и последовательное уменьшение потерь на каждой эпохе. 
- Постепенное и последовательное уменьшение потерь свидетельствует о надежности и предсказуемости SGD.
- Хорошо подходит для задач, где стабильность и предсказуемость важны, особенно на начальных этапах обучения.

### Выводы
- Предпочтительность алгоритма: Исходя из графиков, если основная цель — максимальная стабильность и предсказуемость при снижении потерь на небольшом числе эпох, предпочтительным вариантом станет SGD. Однако если задача требует более тонкой настройки и допускает колебания на начальных этапах, Rprop может оказаться подходящим.

![loss_plot_Rprop.png](loss_plot_Rprop.png)
![loss_plot_SGD.png](loss_plot_SGD.png)

# Задание 3 

Оптимизация гиперпараметров с помощью optune.

Датасет FashionMnist был заменен на датасет CIFAR-10

Поиск должен был осуществляться по данным гиперпараметра: ("dropout", 0.2, 0.5) ; ("optimizer", ["Adam", "RMSprop", "SGD"])


Вывод консоли:
```Study statistics: 
  Number of finished trials:  46
  Number of pruned trials:  35
  Number of complete trials:  11
Best trial:
  Value:  0.53359375
  Params: 
    n_layers: 2
    n_units_l0: 256
    dropout_l0: 0.30550577988150257
    n_units_l1: 365
    dropout_l1: 0.30404827709049087
    optimizer: Adam
    lr: 0.00343815733716052
```